# configuration file for service orchestrator (currently just hadoop as a
# service)
# Everything has to be within the section "cluster" because of the SO's 
# implementation. All data can also be gived at instantiation. If done so, that 
# data will supersede the data given in this file. On the other hand, if the 
# required data is neither passed during instantiation nor in this file, a 
# default value will be set by the SO which might not be according to the 
# user's needs.
# NOTE: there is an entry (icclab.haas.rootfolder) which cannot be included in
# this file as it is also needed in order to find this very file. It has to be
# delivered to the SO as an attribute additionally to every deployment call!

[cluster]

# default name of the whole cluster; a timestamp will be added for making it
# unique
icclab.haas.cluster.name: distributedprocessing

# image files to take as base image for master and slaves; it should be a 
# debian distribution with user ec2-user and passwordless sudo execution
icclab.haas.master.image: Ubuntu-Trusty-1404-7-10-2015
icclab.haas.slave.image: Ubuntu-Trusty-1404-7-10-2015

# Name of SSH public key registration in keystone; if a name is given without a 
# public key, this key name will be included into the master - in this case, 
# the key has to be registered on keystone already. If key name is given with a 
# public key, a key name with the given public key will be inserted into 
# keystone. (and the master) If neither a key name nor a public key are given, 
# the public key from the file master.id_rsa.pub will be inserted into the 
# master.
icclab.haas.master.sshkeyname:
icclab.haas.master.publickey:

# flavor for master / slave
icclab.haas.master.flavor: m1.small
icclab.haas.slave.flavor: m1.small

# number of masters and slaves to be configured (possibly certain frameworks 
# don't allow multiple masters; plus decision whether a slave node should be 
# started on the master node as well
icclab.haas.master.number: 1
icclab.haas.slave.number: 2
icclab.haas.master.slaveonmaster: True

# should a floating IP be created for the cluster?
icclab.haas.master.withfloatingip: True
icclab.haas.master.attachfloatingipwithid: 034f34b7-5ff5-4109-9e8b-a9d919e7ff39

# name for the master(s) and slave(s)
icclab.haas.master.name: masternode
icclab.haas.slave.name: slavenode

# network configuration for the subnet
icclab.haas.network.subnet.cidr: 192.168.19.0/24
icclab.haas.network.gw.ip: 192.168.19.1
icclab.haas.network.subnet.allocpool.start: 192.168.19.2
icclab.haas.network.subnet.allocpool.end: 192.168.19.254
icclab.haas.network.dnsservers: ["64.6.64.6","64.6.65.6"]

# id of the nova image containing the directory structure for the software to
# be installed (can be retrieved with `nova image-list` or from Horizon)
icclab.haas.master.imageid: 8d92eb3d-2347-4177-939d-4f0f8537d31a

# method how to transfer the files to the slaves of the cluster: either
# transfer them in a packed form and unpack them on the slaves or transfer them
# in an unpacked way; this option is the name of a defined bash function.
# Currently, the two optinos transferUnpackedFiles and transferFirstUnpackLater
# are possible.
icclab.haas.master.transfermethod: transferUnpackedFiles

# some information about the Linux user has to be provided - this user has to
# be present on all of the deployed machines and it needs to have passwordless
# sudo access
icclab.haas.cluster.username: ec2-user
icclab.haas.cluster.usergroup: ec2-user
icclab.haas.cluster.homedir: /home/ec2-user

# debug settings - you can decide on your own whether to deploy the created
# template on OpenStack and for debugging purposes, you can also provide a path
# for saving the template locally on the machine where the SO is run
icclab.haas.debug.donotdeploy: True
icclab.haas.debug.savetemplatetolocalpath:

# should a new volume be created from the volume with the given ID? for
# debugging purposes, it's easier not to create a new volume as it takes more
# time and it strains the system without necessity.
icclab.haas.master.createvolumeforattachment: True